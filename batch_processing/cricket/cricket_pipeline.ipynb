{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import component\n",
    "from kfp import compiler\n",
    "from google.cloud import aiplatform\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"europe-west3-docker.pkg.dev/bda-gameon-demo/vertex/base_cricket_container:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def load_and_preprocess(\n",
    "    match_ids: List[int],\n",
    "    write_disp: str = 'APPEND',\n",
    "):\n",
    "    from cricket_utils import (\n",
    "        load_data,\n",
    "        transform_cricket_data,\n",
    "        save_historic_to_big_query,\n",
    "    )\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(match_ids)\n",
    "\n",
    "    print(\"Transforming data...\")\n",
    "    df = transform_cricket_data(df)\n",
    "\n",
    "    print(\"Saving data to BigQuery...\")\n",
    "    save_historic_to_big_query(df, write_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def train_model():\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "\n",
    "    query = \"\"\"    \n",
    "        CREATE OR REPLACE MODEL cricket.xgboost_model\n",
    "        OPTIONS(model_type='BOOSTED_TREE_CLASSIFIER', \n",
    "                input_label_cols=['winner']) AS\n",
    "        SELECT \n",
    "        inning,\n",
    "        `over` as over_number,\n",
    "        cumulative_score,\n",
    "        cumulative_wickets,\n",
    "        toss_decision,\n",
    "        toss_winner,\n",
    "        season,\n",
    "        current_team,\n",
    "        first_inning_total_score,\n",
    "        first_inning_total_wickets,\n",
    "        first_inning_run_rate,\n",
    "        team_1,\n",
    "        team_2,\n",
    "        run_rate,\n",
    "        required_run_rate,\n",
    "        winner\n",
    "        FROM cricket.historic_data;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Initializing BigQuery client...\")\n",
    "        client = bigquery.Client(project=\"bda-gameon-demo\")\n",
    "        logging.info(\"Running query:\\n%s\", query)\n",
    "        query_job = client.query(query)\n",
    "        query_job.result()\n",
    "        logging.info(\"Query completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failed to execute query: %s\", str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def write_metrics_to_bq():\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "\n",
    "    model_name = \"bda-gameon-demo.cricket.xgboost_model\"\n",
    "    table_id = \"bda-gameon-demo.cricket.model_metrics\"\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Initializing BigQuery client...\")\n",
    "        client = bigquery.Client(project=\"bda-gameon-demo\")\n",
    "\n",
    "        logging.info(\"Checking if table exists: %s\", table_id)\n",
    "        dataset_id, table_name = table_id.split('.')[-2:]\n",
    "        dataset = client.dataset(dataset_id)\n",
    "        table = dataset.table(table_name)\n",
    "\n",
    "        try:\n",
    "            client.get_table(table)\n",
    "            logging.info(\"Table exists: %s\", table_id)\n",
    "        except Exception:\n",
    "            logging.info(\"Table does not exist. Creating table: %s\", table_id)\n",
    "            schema = [\n",
    "                bigquery.SchemaField(\"metric_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "                bigquery.SchemaField(\"metric_value\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "            ]\n",
    "            table = bigquery.Table(table_id, schema=schema)\n",
    "            client.create_table(table)\n",
    "            logging.info(\"Table created successfully: %s\", table_id)\n",
    "\n",
    "        eval_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM ML.EVALUATE(MODEL `{model_name}`)\n",
    "        \"\"\"\n",
    "        logging.info(\"Retrieving evaluation metrics...\")\n",
    "        eval_results = client.query(eval_query).result()\n",
    "\n",
    "        rows_to_insert = []\n",
    "        for row in eval_results:\n",
    "            rows_to_insert.append({\n",
    "                \"metric_name\": row.metric_name,\n",
    "                \"metric_value\": row.metric_value\n",
    "            })\n",
    "\n",
    "        # Write metrics to BigQuery table\n",
    "        logging.info(\"Writing metrics to BigQuery table: %s\", table_id)\n",
    "        errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "        if errors:\n",
    "            logging.error(\"Errors occurred while inserting rows: %s\", errors)\n",
    "            raise Exception(f\"Failed to write metrics: {errors}\")\n",
    "        else:\n",
    "            logging.info(\"Metrics successfully written to BigQuery.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failed to write metrics to BigQuery: %s\", str(e))\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"batch_processing\", description=\"Cricket batch processing pipeline\")\n",
    "def cricket_batch_processing_pipeline(match_ids: List[int] = [i for i in range(1, 30)], write_disp: str = 'APPEND'):\n",
    "    \n",
    "    load_preprocess_step = load_and_preprocess(\n",
    "        match_ids=match_ids, \n",
    "        write_disp=write_disp\n",
    "    ).set_display_name(\"Load and Preprocess Data\")\n",
    "\n",
    "    train_step = train_model().after(load_preprocess_step).set_display_name('Train Model')\n",
    "\n",
    "    write_metrics_step = write_metrics_to_bq().after(train_step).set_display_name('Write Model Metrics to BigQuery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=cricket_batch_processing_pipeline,\n",
    "    package_path=\"cricket_batch_processing_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west3/pipelines/runs/batch-processing-20250104225145?project=248863766350\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/248863766350/locations/europe-west3/pipelineJobs/batch-processing-20250104225145 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [write-metrics-to-bq].; Job (project_id = bda-gameon-demo, job_id = 5320134345552297984) is failed due to the above error.; Failed to handle the job: {project_number = 248863766350, job_id = 5320134345552297984}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m aiplatform\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbda-gameon-demo\u001b[39m\u001b[38;5;124m\"\u001b[39m, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meurope-west3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[0;32m      4\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcricket_batch_processing_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcricket_batch_processing_pipeline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     },\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:334\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[1;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[1;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\cloud\\aiplatform\\base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[0;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:382\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[1;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    375\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[0;32m    376\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m     enable_preflight_validations\u001b[38;5;241m=\u001b[39menable_preflight_validations,\n\u001b[0;32m    380\u001b[0m )\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:793\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [write-metrics-to-bq].; Job (project_id = bda-gameon-demo, job_id = 5320134345552297984) is failed due to the above error.; Failed to handle the job: {project_number = 248863766350, job_id = 5320134345552297984}\"\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=\"bda-gameon-demo\", location=\"europe-west3\")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"cricket_batch_processing_pipeline\",\n",
    "    template_path=\"cricket_batch_processing_pipeline.json\",\n",
    "    parameter_values={\n",
    "        \"match_ids\": [2363],\n",
    "        \"write_disp\": 'APPEND',\n",
    "    },\n",
    ")\n",
    "\n",
    "pipeline_job.run(sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Invalid table-valued function ML.PREDICT\nColumn over_number with type INT64 cannot be converted to type STRING from training implicitly according to the coercion rule: https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules. at [3:6]; reason: invalidQuery, location: query, message: Invalid table-valued function ML.PREDICT\nColumn over_number with type INT64 cannot be converted to type STRING from training implicitly according to the coercion rule: https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules. at [3:6]\n\nLocation: EU\nJob ID: 9ecdbb09-01b9-4d39-82e6-baf53a059940\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 47\u001b[0m\n\u001b[0;32m     24\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124mSELECT winner\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124mFROM ML.PREDICT(MODEL `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, (\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124m))\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     46\u001b[0m query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(query)\n\u001b[1;32m---> 47\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1681\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[1;32m-> 1681\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1682\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\izate\\anaconda3\\envs\\dl2\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1630\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m     restart_query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[0;32m   1640\u001b[0m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_query_results(\n\u001b[0;32m   1642\u001b[0m         retry\u001b[38;5;241m=\u001b[39mretry, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreload_query_results_kwargs\n\u001b[0;32m   1643\u001b[0m     )\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 Invalid table-valued function ML.PREDICT\nColumn over_number with type INT64 cannot be converted to type STRING from training implicitly according to the coercion rule: https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules. at [3:6]; reason: invalidQuery, location: query, message: Invalid table-valued function ML.PREDICT\nColumn over_number with type INT64 cannot be converted to type STRING from training implicitly according to the coercion rule: https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules. at [3:6]\n\nLocation: EU\nJob ID: 9ecdbb09-01b9-4d39-82e6-baf53a059940\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "PROJECT_ID = \"bda-gameon-demo\"\n",
    "MODEL = \"cricket.xgboost_model\"\n",
    "row = {\n",
    "    \"inning\": 1,\n",
    "    \"over_number\": 1,\n",
    "    \"cumulative_score\": 0,\n",
    "    \"cumulative_wickets\": 0,\n",
    "    \"toss_decision\": 1,\n",
    "    \"toss_winner\": 1,\n",
    "    \"season\": 2021,\n",
    "    \"current_team\": 1,\n",
    "    \"first_inning_total_score\": 0,\n",
    "    \"first_inning_total_wickets\": 0,\n",
    "    \"first_inning_run_rate\": 0,\n",
    "    \"team_1\": \"A\",\n",
    "    \"team_2\": \"B\",\n",
    "    \"run_rate\": 0,\n",
    "    \"required_run_rate\": 0,\n",
    "}\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT winner\n",
    "FROM ML.PREDICT(MODEL `{MODEL}`, (\n",
    "    SELECT \n",
    "    {row['inning']} AS inning,\n",
    "    {row['over_number']} AS over_number,\n",
    "    {row['cumulative_score']} AS cumulative_score,\n",
    "    {row['cumulative_wickets']} AS cumulative_wickets,\n",
    "    {row['toss_decision']} AS toss_decision,\n",
    "    {row['toss_winner']} AS toss_winner,\n",
    "    {row['season']} AS season,\n",
    "    {row['current_team']} AS current_team,\n",
    "    {row['first_inning_total_score']} AS first_inning_total_score,\n",
    "    {row['first_inning_total_wickets']} AS first_inning_total_wickets,\n",
    "    {row['first_inning_run_rate']} AS first_inning_run_rate,\n",
    "    '{row['team_1']}' AS team_1,\n",
    "    '{row['team_2']}' AS team_2,\n",
    "    {row['run_rate']} AS run_rate,\n",
    "    {row['required_run_rate']} AS required_run_rate\n",
    "))\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = list(query_job.result())\n",
    "print(f\"Prediction: {results[0]['winner']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
